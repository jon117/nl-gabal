======================================================================
PERSISTENT LSTM STATES EXPERIMENT
======================================================================

Testing impact of persistent LSTM states on long sequences
======================================================================
Using device: cuda
GPU: NVIDIA GeForce RTX 4090 D

Downloading WikiText-2...
Using cached WikiText-2: /workspace/nl-gabal/ai-notes/wikitext2_train.txt
Downloaded 10,780,437 characters
Vocabulary: 283 characters
Dataset: 10,780,437 tokens

Created 1314 batches of size 32 x 256

======================================================================
EXPERIMENT: 1_baseline_reset_every_batch
======================================================================
Persistent states: False
Use surprise: False
Sequence length: 256
Steps: 2000
======================================================================
Random seed set to: 42
ChunkedUpdateScheduler initialized with chunk sizes: {'level1_fast': 1, 'level2_medium': 16, 'level3_slow': 256}

======================================================================
Setting up optimizers with scaled learning rates
======================================================================
Level                Chunk Size   Base LR      Scaled LR   
----------------------------------------------------------------------
level1_fast          1            3.00e-04     3.00e-04    
level2_medium        16           3.00e-04     1.87e-05    
level3_slow          256          3.00e-04     1.17e-06    
======================================================================

Training for 2000 steps...
  Step 200/2000 | Loss: 5.1074 | PPL: 165.24 | Aux: 0.0000 | Time: 3.6s
  Step 400/2000 | Loss: 4.8271 | PPL: 124.85 | Aux: 0.0000 | Time: 7.2s
  Step 600/2000 | Loss: 4.7027 | PPL: 110.24 | Aux: 0.0000 | Time: 10.8s
  Step 800/2000 | Loss: 4.5857 | PPL: 98.07 | Aux: 0.0000 | Time: 14.3s
  Step 1000/2000 | Loss: 4.4788 | PPL: 88.13 | Aux: 0.0000 | Time: 17.9s
  Step 1200/2000 | Loss: 4.3708 | PPL: 79.10 | Aux: 0.0000 | Time: 21.7s
  Step 1400/2000 | Loss: 4.2669 | PPL: 71.30 | Aux: 0.0000 | Time: 25.5s
  Step 1600/2000 | Loss: 4.1690 | PPL: 64.65 | Aux: 0.0000 | Time: 29.2s
  Step 1800/2000 | Loss: 4.0678 | PPL: 58.43 | Aux: 0.0000 | Time: 33.0s
  Step 2000/2000 | Loss: 3.9841 | PPL: 53.74 | Aux: 0.0000 | Time: 36.7s

======================================================================
RESULTS: 1_baseline_reset_every_batch
======================================================================
Final perplexity: 52.58
Total time: 36.7s
======================================================================

Created 1314 batches of size 32 x 256

======================================================================
EXPERIMENT: 2_persistent_states_only
======================================================================
Persistent states: True
Use surprise: False
Sequence length: 256
Steps: 2000
======================================================================
Random seed set to: 42
ChunkedUpdateScheduler initialized with chunk sizes: {'level1_fast': 1, 'level2_medium': 16, 'level3_slow': 256}

======================================================================
Setting up optimizers with scaled learning rates
======================================================================
Level                Chunk Size   Base LR      Scaled LR   
----------------------------------------------------------------------
level1_fast          1            3.00e-04     3.00e-04    
level2_medium        16           3.00e-04     1.87e-05    
level3_slow          256          3.00e-04     1.17e-06    
======================================================================

Training for 2000 steps...
Traceback (most recent call last):
  File "/workspace/nl-gabal/ai-notes/persistent_state_experiment.py", line 437, in <module>
    main()
  File "/workspace/nl-gabal/ai-notes/persistent_state_experiment.py", line 378, in main
    result = run_experiment(
             ^^^^^^^^^^^^^^^
  File "/workspace/nl-gabal/ai-notes/persistent_state_experiment.py", line 272, in run_experiment
    final_ppl, losses, total_time = train_model(
                                    ^^^^^^^^^^^^
  File "/workspace/nl-gabal/ai-notes/persistent_state_experiment.py", line 152, in train_model
    logits, surprise_info = model(x_emb, compute_surprise=compute_surprise)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/openhands/micromamba/envs/openhands/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/openhands/micromamba/envs/openhands/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/nl-gabal/src/model_state.py", line 181, in forward
    if self.track_surprise and compute_surprise:
       ^^^^^^^^^^^^^^^^^^^
  File "/openhands/micromamba/envs/openhands/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1964, in __getattr__
    raise AttributeError(
AttributeError: 'NestedModelWithState' object has no attribute 'track_surprise'
