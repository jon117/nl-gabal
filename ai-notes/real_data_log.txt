
======================================================================
REAL DATA EXPERIMENTS
======================================================================

Testing surprise objectives on structured language data
This is the REAL test - will surprise help?

Using device: cuda (NVIDIA GeForce RTX 4090 D)
Downloading WikiText-2...
Downloaded 10,780,437 characters

Preparing dataset...
Vocabulary size: 283 characters
Dataset size: 10,780,437 tokens

======================================================================
EXPERIMENT: baseline
======================================================================
Use surprise: False
Surprise weights: {}
Steps: 2000
======================================================================

Random seed set to: 42
Model parameters: 1,197,339
ChunkedUpdateScheduler initialized with chunk sizes: {'level1_fast': 1, 'level2_medium': 16, 'level3_slow': 256}

======================================================================
Setting up optimizers with scaled learning rates
======================================================================
Level                Chunk Size   Base LR      Scaled LR   
----------------------------------------------------------------------
level1_fast          1            3.00e-04     3.00e-04    
level2_medium        16           3.00e-04     1.87e-05    
level3_slow          256          3.00e-04     1.17e-06    
======================================================================


======================================================================
SurpriseLossComputer initialized
======================================================================
Loss weights: {'level1_fast': 0.3, 'level2_medium': 0.1}
Gradient clip value: 10.0
Compute surprise every 1 steps
======================================================================

Training for 2000 steps...
  Step 200/2000 | Loss: 5.0654 | PPL: 162.62 | Aux: 0.0000 | Time: 1.7s
  Step 400/2000 | Loss: 4.8104 | PPL: 122.88 | Aux: 0.0000 | Time: 3.1s
  Step 600/2000 | Loss: 4.6882 | PPL: 108.74 | Aux: 0.0000 | Time: 4.6s
  Step 800/2000 | Loss: 4.5753 | PPL: 97.12 | Aux: 0.0000 | Time: 6.0s
  Step 1000/2000 | Loss: 4.4675 | PPL: 87.21 | Aux: 0.0000 | Time: 7.5s
  Step 1200/2000 | Loss: 4.3687 | PPL: 79.00 | Aux: 0.0000 | Time: 8.9s
  Step 1400/2000 | Loss: 4.2704 | PPL: 71.60 | Aux: 0.0000 | Time: 10.0s
  Step 1600/2000 | Loss: 4.1741 | PPL: 65.03 | Aux: 0.0000 | Time: 10.8s
  Step 1800/2000 | Loss: 4.0833 | PPL: 59.40 | Aux: 0.0000 | Time: 11.7s
  Step 2000/2000 | Loss: 4.0026 | PPL: 54.79 | Aux: 0.0000 | Time: 12.7s

======================================================================
RESULTS: baseline
======================================================================
Final loss: 3.9808
Final perplexity: 53.60
PPL improvement: 187.48 → 53.60 (133.88)
Total time: 12.7s (6.3ms/step)
======================================================================


======================================================================
EXPERIMENT: surprise_low
======================================================================
Use surprise: True
Surprise weights: {'level1_fast': 0.01, 'level2_medium': 0.005}
Steps: 2000
======================================================================

Random seed set to: 42
Model parameters: 1,197,339
ChunkedUpdateScheduler initialized with chunk sizes: {'level1_fast': 1, 'level2_medium': 16, 'level3_slow': 256}

======================================================================
Setting up optimizers with scaled learning rates
======================================================================
Level                Chunk Size   Base LR      Scaled LR   
----------------------------------------------------------------------
level1_fast          1            3.00e-04     3.00e-04    
level2_medium        16           3.00e-04     1.87e-05    
level3_slow          256          3.00e-04     1.17e-06    
======================================================================


======================================================================
SurpriseLossComputer initialized
======================================================================
Loss weights: {'level1_fast': 0.01, 'level2_medium': 0.005}
Gradient clip value: 10.0
Compute surprise every 1 steps
======================================================================

Training for 2000 steps...
  Step 200/2000 | Loss: 5.0637 | PPL: 162.37 | Aux: 0.0098 | Time: 2.0s
  Step 400/2000 | Loss: 4.8064 | PPL: 122.40 | Aux: 0.0123 | Time: 3.8s
  Step 600/2000 | Loss: 4.6831 | PPL: 108.19 | Aux: 0.0130 | Time: 5.7s
  Step 800/2000 | Loss: 4.5689 | PPL: 96.50 | Aux: 0.0137 | Time: 7.6s
  Step 1000/2000 | Loss: 4.4603 | PPL: 86.58 | Aux: 0.0144 | Time: 9.5s
  Step 1200/2000 | Loss: 4.3609 | PPL: 78.39 | Aux: 0.0152 | Time: 11.3s
  Step 1400/2000 | Loss: 4.2622 | PPL: 71.02 | Aux: 0.0161 | Time: 13.2s
  Step 1600/2000 | Loss: 4.1658 | PPL: 64.50 | Aux: 0.0171 | Time: 15.2s
  Step 1800/2000 | Loss: 4.0758 | PPL: 58.95 | Aux: 0.0181 | Time: 16.9s
  Step 2000/2000 | Loss: 3.9958 | PPL: 54.42 | Aux: 0.0191 | Time: 18.7s

======================================================================
RESULTS: surprise_low
======================================================================
Final loss: 3.9741
Final perplexity: 53.24
PPL improvement: 187.34 → 53.24 (134.09)
Total time: 18.7s (9.4ms/step)
======================================================================


======================================================================
EXPERIMENT: surprise_medium
======================================================================
Use surprise: True
Surprise weights: {'level1_fast': 0.05, 'level2_medium': 0.01}
Steps: 2000
======================================================================

Random seed set to: 42
Model parameters: 1,197,339
ChunkedUpdateScheduler initialized with chunk sizes: {'level1_fast': 1, 'level2_medium': 16, 'level3_slow': 256}

======================================================================
Setting up optimizers with scaled learning rates
======================================================================
Level                Chunk Size   Base LR      Scaled LR   
----------------------------------------------------------------------
level1_fast          1            3.00e-04     3.00e-04    
level2_medium        16           3.00e-04     1.87e-05    
level3_slow          256          3.00e-04     1.17e-06    
======================================================================


======================================================================
SurpriseLossComputer initialized
======================================================================
Loss weights: {'level1_fast': 0.05, 'level2_medium': 0.01}
Gradient clip value: 10.0
Compute surprise every 1 steps
======================================================================

Training for 2000 steps...
  Step 200/2000 | Loss: 5.0579 | PPL: 161.58 | Aux: 0.0381 | Time: 1.9s
  Step 400/2000 | Loss: 4.7920 | PPL: 120.65 | Aux: 0.0470 | Time: 3.8s
  Step 600/2000 | Loss: 4.6680 | PPL: 106.56 | Aux: 0.0484 | Time: 5.7s
  Step 800/2000 | Loss: 4.5531 | PPL: 95.00 | Aux: 0.0498 | Time: 7.5s
  Step 1000/2000 | Loss: 4.4451 | PPL: 85.27 | Aux: 0.0514 | Time: 9.4s
  Step 1200/2000 | Loss: 4.3457 | PPL: 77.20 | Aux: 0.0531 | Time: 11.3s
  Step 1400/2000 | Loss: 4.2457 | PPL: 69.86 | Aux: 0.0547 | Time: 13.2s
  Step 1600/2000 | Loss: 4.1474 | PPL: 63.32 | Aux: 0.0564 | Time: 14.2s
  Step 1800/2000 | Loss: 4.0546 | PPL: 57.72 | Aux: 0.0584 | Time: 15.3s
  Step 2000/2000 | Loss: 3.9743 | PPL: 53.26 | Aux: 0.0605 | Time: 16.3s

======================================================================
RESULTS: surprise_medium
======================================================================
Final loss: 3.9527
Final perplexity: 52.12
PPL improvement: 186.96 → 52.12 (134.84)
Total time: 16.3s (8.2ms/step)
======================================================================


======================================================================
FINAL COMPARISON
======================================================================

Experiment           Final PPL    PPL Δ        Time (s)  
----------------------------------------------------------------------
baseline             53.60              +0.00 12.7      
surprise_low         53.24              +0.36 18.7      
surprise_medium      52.12              +1.48 16.3      
----------------------------------------------------------------------

======================================================================
ANALYSIS
======================================================================

Best configuration: surprise_medium
  Final perplexity: 52.12
  Improvement over baseline: +1.48 PPL

✅ Surprise objectives HELPED!
   Best surprise config: surprise_medium
   Improvement: 1.48 PPL (2.8%)

Results saved to: /workspace/nl-gabal/ai-notes/real_data_results.json

======================================================================
EXPERIMENTS COMPLETE!
======================================================================
