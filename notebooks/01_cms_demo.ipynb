{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efabc4bf",
   "metadata": {},
   "source": [
    "# Nested Learning: Continuum Memory System (CMS) Demo\n",
    "\n",
    "This notebook demonstrates the **Tier 1** implementation of Nested Learning from the Google paper.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "The Continuum Memory System implements **multi-timescale learning** where different parameter groups update at different frequencies:\n",
    "\n",
    "- **Level 1 (Fast)**: Updates every step â†’ captures short-term patterns\n",
    "- **Level 2 (Medium)**: Updates every 16 steps â†’ captures mid-term patterns  \n",
    "- **Level 3 (Slow)**: Updates every 256 steps â†’ captures long-term structure\n",
    "\n",
    "This is achieved through:\n",
    "1. **Step-aligned gradient accumulation**: Native PyTorch gradient accumulation\n",
    "2. **Selective gradient zeroing**: Only zero gradients for levels that just updated\n",
    "3. **Learning rate scaling**: Scale LR by 1/chunk_size to compensate for accumulated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from model import NestedModel\n",
    "from scheduler import ChunkedUpdateScheduler\n",
    "from utils import setup_optimizers, set_seed, create_dummy_data\n",
    "\n",
    "# Set random seed\n",
    "set_seed(42)\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d631a",
   "metadata": {},
   "source": [
    "## 1. Initialize the Model\n",
    "\n",
    "The `NestedModel` is a standard PyTorch module with parameters organized into three levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d78170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = NestedModel(input_size=768, hidden_size=3072)\n",
    "model.print_model_info()\n",
    "\n",
    "# Verify levels are accessible\n",
    "print(\"\\nLevel names:\", model.get_level_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04f87d",
   "metadata": {},
   "source": [
    "## 2. Initialize the Scheduler\n",
    "\n",
    "The `ChunkedUpdateScheduler` orchestrates when each level updates using **step-aligned logic**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b29081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk sizes (update frequencies)\n",
    "chunk_sizes = {\n",
    "    \"level1_fast\": 1,      # Updates every step\n",
    "    \"level2_medium\": 16,   # Updates every 16 steps\n",
    "    \"level3_slow\": 256,    # Updates every 256 steps\n",
    "}\n",
    "\n",
    "scheduler = ChunkedUpdateScheduler(chunk_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85721e7",
   "metadata": {},
   "source": [
    "### Visualize Update Schedule\n",
    "\n",
    "Let's see when each level updates over the first 300 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cccb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate which levels update at each step\n",
    "steps = range(1, 301)\n",
    "update_pattern = {level: [] for level in chunk_sizes.keys()}\n",
    "\n",
    "for step in steps:\n",
    "    for level_name in chunk_sizes.keys():\n",
    "        if scheduler.should_update(level_name, step):\n",
    "            update_pattern[level_name].append(step)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors = {'level1_fast': 'red', 'level2_medium': 'blue', 'level3_slow': 'green'}\n",
    "y_positions = {'level1_fast': 3, 'level2_medium': 2, 'level3_slow': 1}\n",
    "\n",
    "for level_name, steps_updated in update_pattern.items():\n",
    "    y = [y_positions[level_name]] * len(steps_updated)\n",
    "    ax.scatter(steps_updated, y, c=colors[level_name], s=10, alpha=0.6, label=level_name)\n",
    "\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['Slow (256)', 'Medium (16)', 'Fast (1)'])\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Learning Level')\n",
    "ax.set_title('Update Schedule: When Each Level Updates')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Updates in first 300 steps:\")\n",
    "for level_name, steps_updated in update_pattern.items():\n",
    "    print(f\"  {level_name:20s}: {len(steps_updated):3d} updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5555815",
   "metadata": {},
   "source": [
    "## 3. Setup Optimizers with Scaled Learning Rates\n",
    "\n",
    "**CRITICAL**: Learning rates must be scaled by `1/chunk_size` to compensate for gradient accumulation.\n",
    "\n",
    "Why? Gradients accumulate (sum) over multiple steps. Without LR scaling, slower levels would take massive steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 1e-4\n",
    "\n",
    "optimizers = setup_optimizers(\n",
    "    model=model,\n",
    "    chunk_sizes=chunk_sizes,\n",
    "    base_lr=base_lr,\n",
    "    optimizer_type=\"adam\",\n",
    "    weight_decay=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650a5e7",
   "metadata": {},
   "source": [
    "### Effective Batch Size\n",
    "\n",
    "The learning rate scaling makes each level operate as if it's using a different batch size:\n",
    "\n",
    "| Level | Chunk Size | Scaled LR | Effective Batch Size |\n",
    "|-------|------------|-----------|----------------------|\n",
    "| Fast  | 1          | 1e-4      | 32 (base)           |\n",
    "| Medium| 16         | 6.25e-6   | 512 (32 Ã— 16)       |\n",
    "| Slow  | 256        | 3.9e-7    | 8192 (32 Ã— 256)     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effective batch sizes\n",
    "base_batch_size = 32\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Learning rates\n",
    "levels = list(chunk_sizes.keys())\n",
    "lrs = [base_lr / chunk_sizes[level] for level in levels]\n",
    "ax1.bar(range(len(levels)), lrs, color=['red', 'blue', 'green'], alpha=0.7)\n",
    "ax1.set_xticks(range(len(levels)))\n",
    "ax1.set_xticklabels(['Fast', 'Medium', 'Slow'])\n",
    "ax1.set_ylabel('Learning Rate')\n",
    "ax1.set_title('Scaled Learning Rates')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Effective batch sizes\n",
    "eff_batch_sizes = [base_batch_size * chunk_sizes[level] for level in levels]\n",
    "ax2.bar(range(len(levels)), eff_batch_sizes, color=['red', 'blue', 'green'], alpha=0.7)\n",
    "ax2.set_xticks(range(len(levels)))\n",
    "ax2.set_xticklabels(['Fast', 'Medium', 'Slow'])\n",
    "ax2.set_ylabel('Effective Batch Size')\n",
    "ax2.set_title('Effective Batch Sizes (via Gradient Accumulation)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0326e9",
   "metadata": {},
   "source": [
    "## 4. Training Loop Demo\n",
    "\n",
    "Let's run a small training loop to see the system in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205995b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Track gradient norms and updates\n",
    "grad_norms = defaultdict(list)\n",
    "update_steps = defaultdict(list)\n",
    "\n",
    "num_steps = 300\n",
    "print(f\"Running {num_steps} training steps...\\n\")\n",
    "\n",
    "for global_step in range(1, num_steps + 1):\n",
    "    # Generate dummy data\n",
    "    data, targets = create_dummy_data(\n",
    "        batch_size=32,\n",
    "        seq_length=128,\n",
    "        input_size=768,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Forward & Backward\n",
    "    output = model(data)\n",
    "    loss = criterion(output, data)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Track gradient norms BEFORE potential zeroing\n",
    "    for level_name, module in model.levels.items():\n",
    "        grad_norm = 0.0\n",
    "        for p in module.parameters():\n",
    "            if p.grad is not None:\n",
    "                grad_norm += p.grad.norm().item() ** 2\n",
    "        grad_norms[level_name].append(np.sqrt(grad_norm))\n",
    "    \n",
    "    # Selective Update & Gradient Zeroing\n",
    "    for level_name, module in model.levels.items():\n",
    "        if scheduler.should_update(level_name, global_step):\n",
    "            optimizers[level_name].step()\n",
    "            scheduler.mark_updated(level_name, global_step)\n",
    "            update_steps[level_name].append(global_step)\n",
    "            \n",
    "            # Zero only this level's gradients\n",
    "            for p in module.parameters():\n",
    "                if p.grad is not None:\n",
    "                    p.grad.zero_()\n",
    "    \n",
    "    if global_step % 100 == 0:\n",
    "        print(f\"Step {global_step}/{num_steps} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44b434",
   "metadata": {},
   "source": [
    "## 5. Visualize Gradient Accumulation\n",
    "\n",
    "The key insight: gradient norms grow for slower levels as they accumulate, then reset after updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59298ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "level_info = [\n",
    "    ('level1_fast', 'Fast (updates every step)', 'red', axes[0]),\n",
    "    ('level2_medium', 'Medium (updates every 16 steps)', 'blue', axes[1]),\n",
    "    ('level3_slow', 'Slow (updates every 256 steps)', 'green', axes[2]),\n",
    "]\n",
    "\n",
    "for level_name, title, color, ax in level_info:\n",
    "    steps = range(1, len(grad_norms[level_name]) + 1)\n",
    "    \n",
    "    # Plot gradient norms\n",
    "    ax.plot(steps, grad_norms[level_name], color=color, alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Mark update points\n",
    "    if level_name in update_steps:\n",
    "        update_y = [grad_norms[level_name][s-1] for s in update_steps[level_name]]\n",
    "        ax.scatter(update_steps[level_name], update_y, \n",
    "                  c='black', s=30, marker='v', zorder=5, \n",
    "                  label='Update & Zero Grad')\n",
    "    \n",
    "    ax.set_ylabel('Gradient Norm')\n",
    "    ax.set_title(f'{title}')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Training Step')\n",
    "fig.suptitle('Gradient Accumulation Patterns Across Levels', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how:\")\n",
    "print(\"- Fast level: Gradients stay small (zeroed every step)\")\n",
    "print(\"- Medium level: Gradients accumulate for 16 steps, then reset\")\n",
    "print(\"- Slow level: Gradients accumulate for 256 steps, then reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f63732",
   "metadata": {},
   "source": [
    "## 6. Update Statistics\n",
    "\n",
    "Let's verify the scheduler tracked updates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f228873",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.print_stats(num_steps)\n",
    "\n",
    "# Verify the math\n",
    "print(\"\\nVerification:\")\n",
    "for level_name, chunk_size in chunk_sizes.items():\n",
    "    expected = num_steps // chunk_size\n",
    "    actual = scheduler.get_update_count(level_name)\n",
    "    status = \"âœ“\" if expected == actual else \"âœ—\"\n",
    "    print(f\"{status} {level_name:20s}: Expected {expected}, Got {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a828a",
   "metadata": {},
   "source": [
    "## 7. Memory Efficiency\n",
    "\n",
    "**Key Advantage**: We don't store separate gradient buffers for each level.\n",
    "\n",
    "- Single forward/backward pass computes gradients for ALL parameters\n",
    "- Gradients naturally accumulate in `.grad` attributes  \n",
    "- Selective zeroing enables different timescales\n",
    "- Memory overhead: **~0%** compared to standard training\n",
    "\n",
    "This is why CMS is practical even for large models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79da24c",
   "metadata": {},
   "source": [
    "## Summary: Key Concepts\n",
    "\n",
    "### âœ“ What We Implemented (Tier 1: CMS)\n",
    "\n",
    "1. **NestedModel**: Standard PyTorch module with parameters grouped into levels\n",
    "2. **ChunkedUpdateScheduler**: Step-aligned update logic\n",
    "3. **Scaled Learning Rates**: LR Ã— (1/chunk_size) for each level\n",
    "4. **Selective Gradient Zeroing**: Only zero gradients for levels that updated\n",
    "5. **Memory Efficient**: Uses native PyTorch gradient accumulation\n",
    "\n",
    "### ðŸŽ¯ Core Principles\n",
    "\n",
    "- **Multi-timescale learning**: Different parameters learn at different speeds\n",
    "- **Step-aligned updates**: Updates at specific multiples (16, 32, 48...)\n",
    "- **No computational overhead**: Single forward/backward pass\n",
    "- **Faithful to paper**: Mathematically correct gradient accumulation\n",
    "\n",
    "### ðŸš€ Next Steps: Tier 2 & 3\n",
    "\n",
    "- **Tier 2**: Add auxiliary losses on intermediate activations\n",
    "- **Tier 3**: Implement GABAL (learned learning rates)\n",
    "\n",
    "The current architecture is designed for easy extension!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
